#!/usr/bin/env python3
"""
SEO å„ªåŒ–å·¥å…·
è‡ªå‹•ä¿®å¾©å¸¸è¦‹çš„ SEO å•é¡Œ
"""

import os
import sys
from pathlib import Path
import json
from bs4 import BeautifulSoup
import argparse

class SEOOptimizer:
    def __init__(self, public_dir: str = "public"):
        self.public_dir = Path(public_dir)
        self.fixes = []
        self.suggestions = []
        
    def optimize_meta_descriptions(self, html_content: str, file_path: Path) -> str:
        """å„ªåŒ– meta æè¿°é•·åº¦"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æª¢æŸ¥ meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            content = meta_desc['content']
            if len(content) > 160:
                # æˆªæ–·åˆ° 160 å­—ç¬¦ï¼Œä½†ä¿æŒå®Œæ•´çš„å¥å­
                truncated = content[:157] + "..."
                meta_desc['content'] = truncated
                self.fixes.append(f"{file_path.name}: æˆªæ–·éé•·çš„ meta æè¿°")
                return str(soup)
        
        return html_content
    
    def add_missing_alt_tags(self, html_content: str, file_path: Path) -> str:
        """ç‚ºç¼ºå°‘ alt å±¬æ€§çš„åœ–ç‰‡æ·»åŠ  alt æ¨™ç±¤"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        images = soup.find_all('img')
        for img in images:
            if not img.get('alt'):
                # å˜—è©¦å¾ src æˆ– title ç”Ÿæˆ alt æ–‡å­—
                src = img.get('src', '')
                title = img.get('title', '')
                
                if title:
                    img['alt'] = title
                elif src:
                    # å¾æª”æ¡ˆåç”Ÿæˆ alt æ–‡å­—
                    filename = Path(src).stem
                    alt_text = filename.replace('-', ' ').replace('_', ' ').title()
                    img['alt'] = alt_text
                else:
                    img['alt'] = "åœ–ç‰‡"
                
                self.fixes.append(f"{file_path.name}: ç‚ºåœ–ç‰‡æ·»åŠ  alt å±¬æ€§")
        
        return str(soup)
    
    def optimize_headings(self, html_content: str, file_path: Path) -> str:
        """å„ªåŒ–æ¨™é¡Œçµæ§‹"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ h1 æ¨™ç±¤
        h1_tags = soup.find_all('h1')
        if len(h1_tags) == 0:
            self.suggestions.append(f"{file_path.name}: å»ºè­°æ·»åŠ  H1 æ¨™ç±¤")
        elif len(h1_tags) > 1:
            self.suggestions.append(f"{file_path.name}: ç™¼ç¾å¤šå€‹ H1 æ¨™ç±¤ï¼Œå»ºè­°åªä¿ç•™ä¸€å€‹")
        
        return html_content
    
    def add_structured_data_improvements(self, html_content: str, file_path: Path) -> str:
        """æ”¹é€²çµæ§‹åŒ–è³‡æ–™"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æª¢æŸ¥ JSON-LD
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        
        for script in json_ld_scripts:
            try:
                data = json.loads(script.string)
                
                if data.get('@type') == 'BlogPosting':
                    # æª¢æŸ¥æ˜¯å¦æœ‰åœ–ç‰‡
                    if 'image' not in data:
                        # å˜—è©¦å¾é é¢ä¸­æ‰¾åˆ°ç¬¬ä¸€å¼µåœ–ç‰‡
                        first_img = soup.find('img')
                        if first_img and first_img.get('src'):
                            data['image'] = first_img['src']
                            script.string = json.dumps(data, ensure_ascii=False, indent=2)
                            self.fixes.append(f"{file_path.name}: ç‚ºçµæ§‹åŒ–è³‡æ–™æ·»åŠ åœ–ç‰‡")
                    
                    # æª¢æŸ¥é—œéµå­—
                    if 'keywords' not in data:
                        # å˜—è©¦å¾ meta keywords ç²å–
                        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
                        if meta_keywords and meta_keywords.get('content'):
                            data['keywords'] = meta_keywords['content']
                            script.string = json.dumps(data, ensure_ascii=False, indent=2)
                            self.fixes.append(f"{file_path.name}: ç‚ºçµæ§‹åŒ–è³‡æ–™æ·»åŠ é—œéµå­—")
                            
            except json.JSONDecodeError:
                continue
        
        return str(soup)
    
    def optimize_file(self, file_path: Path) -> bool:
        """å„ªåŒ–å–®å€‹æª”æ¡ˆ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
            
            original_content = html_content
            
            # æ‡‰ç”¨å„ç¨®å„ªåŒ–
            html_content = self.optimize_meta_descriptions(html_content, file_path)
            html_content = self.add_missing_alt_tags(html_content, file_path)
            html_content = self.optimize_headings(html_content, file_path)
            html_content = self.add_structured_data_improvements(html_content, file_path)
            
            # å¦‚æœæœ‰è®Šæ›´ï¼Œå¯«å›æª”æ¡ˆ
            if html_content != original_content:
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(html_content)
                return True
                
        except Exception as e:
            print(f"âŒ è™•ç†æª”æ¡ˆ {file_path} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
        
        return False
    
    def optimize_all(self) -> bool:
        """å„ªåŒ–æ‰€æœ‰æª”æ¡ˆ"""
        if not self.public_dir.exists():
            print(f"âŒ æ‰¾ä¸åˆ° public ç›®éŒ„: {self.public_dir}")
            return False
        
        html_files = list(self.public_dir.rglob("*.html"))
        print(f"ğŸ” æ‰¾åˆ° {len(html_files)} å€‹ HTML æª”æ¡ˆ")
        
        optimized_count = 0
        for file_path in html_files:
            if file_path.name == "404.html":
                continue
                
            if self.optimize_file(file_path):
                optimized_count += 1
        
        print(f"\nğŸ“Š å„ªåŒ–çµæœ:")
        print(f"âœ… å„ªåŒ–äº† {optimized_count} å€‹æª”æ¡ˆ")
        print(f"ğŸ”§ æ‡‰ç”¨äº† {len(self.fixes)} å€‹ä¿®å¾©")
        print(f"ğŸ’¡ ç”¢ç”Ÿäº† {len(self.suggestions)} å€‹å»ºè­°")
        
        if self.fixes:
            print("\nğŸ”§ æ‡‰ç”¨çš„ä¿®å¾©:")
            for fix in self.fixes[:10]:  # åªé¡¯ç¤ºå‰10å€‹
                print(f"  â€¢ {fix}")
            if len(self.fixes) > 10:
                print(f"  ... é‚„æœ‰ {len(self.fixes) - 10} å€‹ä¿®å¾©")
        
        if self.suggestions:
            print("\nğŸ’¡ å„ªåŒ–å»ºè­°:")
            for suggestion in self.suggestions[:10]:  # åªé¡¯ç¤ºå‰10å€‹
                print(f"  â€¢ {suggestion}")
            if len(self.suggestions) > 10:
                print(f"  ... é‚„æœ‰ {len(self.suggestions) - 10} å€‹å»ºè­°")
        
        return True

def main():
    parser = argparse.ArgumentParser(description='SEO å„ªåŒ–å·¥å…·')
    parser.add_argument('--public-dir', default='public', help='public ç›®éŒ„è·¯å¾‘')
    parser.add_argument('--dry-run', action='store_true', help='åªæª¢æŸ¥ä¸ä¿®æ”¹')
    
    args = parser.parse_args()
    
    optimizer = SEOOptimizer(args.public_dir)
    
    print("ğŸš€ é–‹å§‹ SEO å„ªåŒ–...")
    success = optimizer.optimize_all()
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()