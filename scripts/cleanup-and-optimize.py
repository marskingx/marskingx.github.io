#!/usr/bin/env python3
"""
æ¸…ç†å’Œå„ªåŒ–å·¥å…·
ç”¨æ–¼æ¸…ç†é‡è¤‡æª”æ¡ˆã€ä¿®å¾©è·¯å¾‘å•é¡Œï¼Œä¸¦å„ªåŒ–çµæ§‹åŒ–è³‡æ–™
"""

import os
import sys
import shutil
from pathlib import Path
from urllib.parse import unquote
import re
from bs4 import BeautifulSoup
import json

class CleanupOptimizer:
    def __init__(self, public_dir: str = "public"):
        self.public_dir = Path(public_dir)
        self.issues = []
        self.fixes = []
        
    def find_duplicate_files(self):
        """æ‰¾åˆ°é‡è¤‡çš„æª”æ¡ˆ"""
        print("ğŸ” å°‹æ‰¾é‡è¤‡æª”æ¡ˆ...")
        
        # æ”¶é›†æ‰€æœ‰ HTML æª”æ¡ˆ
        files_by_content = {}
        all_files = list(self.public_dir.rglob("*.html"))
        
        for file_path in all_files:
            # è·³éä¸€äº›ç‰¹æ®Šæª”æ¡ˆ
            if file_path.name in ["404.html", "index.html"]:
                continue
                
            # æª¢æŸ¥æª”æ¡ˆåç¨±æ˜¯å¦æœ‰å•é¡Œ
            filename = file_path.name
            parent_dir = file_path.parent.name
            
            # æª¢æŸ¥æ˜¯å¦æœ‰ç·¨ç¢¼å•é¡Œçš„æª”æ¡ˆå
            if any(char in filename for char in ['%', '\\', '//']):
                self.issues.append(f"æª”æ¡ˆåç¨±æœ‰ç·¨ç¢¼å•é¡Œ: {file_path}")
                
            # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡çš„æª”æ¡ˆï¼ˆç›¸åŒå…§å®¹ä½†ä¸åŒè·¯å¾‘ï¼‰
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content_hash = hash(f.read())
                    
                if content_hash in files_by_content:
                    self.issues.append(f"é‡è¤‡æª”æ¡ˆ: {file_path} èˆ‡ {files_by_content[content_hash]}")
                else:
                    files_by_content[content_hash] = file_path
            except Exception as e:
                self.issues.append(f"ç„¡æ³•è®€å–æª”æ¡ˆ {file_path}: {e}")
    
    def find_missing_structured_data(self):
        """æ‰¾åˆ°ç¼ºå°‘çµæ§‹åŒ–è³‡æ–™çš„éƒ¨è½æ ¼æ–‡ç« """
        print("ğŸ” å°‹æ‰¾ç¼ºå°‘çµæ§‹åŒ–è³‡æ–™çš„æ–‡ç« ...")
        
        blog_dir = self.public_dir / "blog"
        if not blog_dir.exists():
            return
            
        missing_data_files = []
        
        for file_path in blog_dir.rglob("index.html"):
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # æª¢æŸ¥æ˜¯å¦æœ‰çµæ§‹åŒ–è³‡æ–™
                soup = BeautifulSoup(content, 'html.parser')
                json_ld_scripts = soup.find_all('script', type='application/ld+json')
                
                if not json_ld_scripts:
                    relative_path = file_path.relative_to(self.public_dir)
                    missing_data_files.append(relative_path)
                    
            except Exception as e:
                self.issues.append(f"ç„¡æ³•æª¢æŸ¥æª”æ¡ˆ {file_path}: {e}")
        
        if missing_data_files:
            print(f"ğŸ“Š æ‰¾åˆ° {len(missing_data_files)} å€‹ç¼ºå°‘çµæ§‹åŒ–è³‡æ–™çš„æ–‡ç« ")
            for file_path in missing_data_files[:10]:  # åªé¡¯ç¤ºå‰10å€‹
                print(f"  â€¢ {file_path}")
            if len(missing_data_files) > 10:
                print(f"  ... é‚„æœ‰ {len(missing_data_files) - 10} å€‹æª”æ¡ˆ")
    
    def analyze_url_patterns(self):
        """åˆ†æ URL æ¨¡å¼ï¼Œæ‰¾å‡ºå•é¡Œ"""
        print("ğŸ” åˆ†æ URL æ¨¡å¼...")
        
        blog_dirs = []
        for item in (self.public_dir / "blog").iterdir():
            if item.is_dir():
                blog_dirs.append(item.name)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ç›¸ä¼¼çš„ç›®éŒ„åç¨±
        similar_dirs = {}
        for dir_name in blog_dirs:
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œç·¨ç¢¼
            clean_name = re.sub(r'[-_\d]+$', '', dir_name)
            clean_name = re.sub(r'^[\d-]+', '', clean_name)
            
            if clean_name in similar_dirs:
                similar_dirs[clean_name].append(dir_name)
            else:
                similar_dirs[clean_name] = [dir_name]
        
        # å ±å‘Šç›¸ä¼¼çš„ç›®éŒ„
        for clean_name, dirs in similar_dirs.items():
            if len(dirs) > 1:
                print(f"âš ï¸  ç›¸ä¼¼çš„ç›®éŒ„: {dirs}")
    
    def generate_cleanup_script(self):
        """ç”Ÿæˆæ¸…ç†è…³æœ¬"""
        script_content = """@echo off
echo ğŸ§¹ é–‹å§‹æ¸…ç†é‡è¤‡å’Œå•é¡Œæª”æ¡ˆ...

REM é€™è£¡æœƒåˆ—å‡ºå»ºè­°çš„æ¸…ç†æ“ä½œ
REM è«‹æ‰‹å‹•æª¢æŸ¥å¾ŒåŸ·è¡Œ

"""
        
        if self.issues:
            script_content += "REM ç™¼ç¾çš„å•é¡Œ:\n"
            for issue in self.issues[:20]:  # é™åˆ¶æ•¸é‡
                script_content += f"REM {issue}\n"
        
        script_content += """
echo âœ… æ¸…ç†å®Œæˆï¼
pause
"""
        
        with open("scripts/cleanup-suggestions.bat", "w", encoding="utf-8") as f:
            f.write(script_content)
        
        print("ğŸ“ å·²ç”Ÿæˆæ¸…ç†å»ºè­°è…³æœ¬: scripts/cleanup-suggestions.bat")
    
    def run_analysis(self):
        """åŸ·è¡Œå®Œæ•´åˆ†æ"""
        print("ğŸš€ é–‹å§‹åˆ†æå’Œå„ªåŒ–...")
        
        if not self.public_dir.exists():
            print(f"âŒ æ‰¾ä¸åˆ° public ç›®éŒ„: {self.public_dir}")
            return False
        
        self.find_duplicate_files()
        self.find_missing_structured_data()
        self.analyze_url_patterns()
        
        print("\n" + "="*60)
        print("ğŸ“Š åˆ†æçµæœ")
        print("="*60)
        print(f"ç™¼ç¾å•é¡Œ: {len(self.issues)}")
        
        if self.issues:
            print("\nå•é¡Œè©³æƒ…:")
            for issue in self.issues[:10]:  # åªé¡¯ç¤ºå‰10å€‹
                print(f"  â€¢ {issue}")
            if len(self.issues) > 10:
                print(f"  ... é‚„æœ‰ {len(self.issues) - 10} å€‹å•é¡Œ")
        
        self.generate_cleanup_script()
        return True

def main():
    optimizer = CleanupOptimizer()
    optimizer.run_analysis()

if __name__ == "__main__":
    main()