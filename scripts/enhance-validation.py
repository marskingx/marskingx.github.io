#!/usr/bin/env python3
"""
å¢å¼·ç‰ˆçµæ§‹åŒ–è³‡æ–™é©—è­‰å·¥å…·
æ–°å¢æ›´å¤šé©—è­‰è¦å‰‡å’Œå„ªåŒ–å»ºè­°
"""

import json
import re
import os
import sys
from pathlib import Path
from urllib.parse import urlparse
import requests
from bs4 import BeautifulSoup
import argparse
from typing import List, Dict, Any
from datetime import datetime

class EnhancedValidator:
    def __init__(self, public_dir: str = "public"):
        self.public_dir = Path(public_dir)
        self.errors = []
        self.warnings = []
        self.suggestions = []
        self.success_count = 0
        
    def validate_seo_optimization(self, data: Dict[Any, Any], file_path: Path) -> bool:
        """é©—è­‰ SEO å„ªåŒ–ç›¸é—œçš„çµæ§‹åŒ–è³‡æ–™"""
        relative_path = file_path.relative_to(self.public_dir)
        is_valid = True
        
        if data.get('@type') == 'BlogPosting':
            # æª¢æŸ¥æ¨™é¡Œé•·åº¦
            headline = data.get('headline', '')
            if len(headline) > 60:
                self.warnings.append(f"{relative_path}: æ¨™é¡Œéé•· ({len(headline)} å­—ç¬¦)ï¼Œå»ºè­°æ§åˆ¶åœ¨60å­—ç¬¦å…§")
            elif len(headline) < 30:
                self.warnings.append(f"{relative_path}: æ¨™é¡ŒéçŸ­ ({len(headline)} å­—ç¬¦)ï¼Œå»ºè­°è‡³å°‘30å­—ç¬¦")
            
            # æª¢æŸ¥æè¿°é•·åº¦
            description = data.get('description', '')
            if description:
                if len(description) > 160:
                    self.warnings.append(f"{relative_path}: æè¿°éé•· ({len(description)} å­—ç¬¦)ï¼Œå»ºè­°æ§åˆ¶åœ¨160å­—ç¬¦å…§")
                elif len(description) < 120:
                    self.warnings.append(f"{relative_path}: æè¿°éçŸ­ ({len(description)} å­—ç¬¦)ï¼Œå»ºè­°è‡³å°‘120å­—ç¬¦")
            else:
                self.errors.append(f"{relative_path}: ç¼ºå°‘æè¿° (description)")
                is_valid = False
            
            # æª¢æŸ¥åœ–ç‰‡
            if 'image' not in data:
                self.warnings.append(f"{relative_path}: å»ºè­°æ·»åŠ ç‰¹è‰²åœ–ç‰‡ä»¥æå‡ SEO")
            
            # æª¢æŸ¥é—œéµå­—
            keywords = data.get('keywords', '')
            if not keywords:
                self.suggestions.append(f"{relative_path}: å»ºè­°æ·»åŠ é—œéµå­— (keywords) ä»¥æå‡ SEO")
            
            # æª¢æŸ¥æ–‡ç« é•·åº¦
            word_count = data.get('wordCount', 0)
            if word_count < 300:
                self.warnings.append(f"{relative_path}: æ–‡ç« å­—æ•¸è¼ƒå°‘ ({word_count} å­—)ï¼Œå»ºè­°è‡³å°‘300å­—")
        
        return is_valid
    
    def validate_performance_optimization(self, data: Dict[Any, Any], file_path: Path) -> bool:
        """é©—è­‰æ•ˆèƒ½å„ªåŒ–ç›¸é—œçš„çµæ§‹åŒ–è³‡æ–™"""
        relative_path = file_path.relative_to(self.public_dir)
        
        # æª¢æŸ¥åœ–ç‰‡ URL
        if 'image' in data:
            image_url = data['image']
            if isinstance(image_url, str):
                if not image_url.startswith('https://'):
                    self.warnings.append(f"{relative_path}: åœ–ç‰‡ URL å»ºè­°ä½¿ç”¨ HTTPS")
                
                # æª¢æŸ¥åœ–ç‰‡æ ¼å¼
                if not any(image_url.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                    self.warnings.append(f"{relative_path}: åœ–ç‰‡æ ¼å¼å»ºè­°ä½¿ç”¨ .jpg, .png æˆ– .webp")
        
        return True
    
    def validate_accessibility(self, data: Dict[Any, Any], file_path: Path) -> bool:
        """é©—è­‰ç„¡éšœç¤™ç›¸é—œçš„çµæ§‹åŒ–è³‡æ–™"""
        relative_path = file_path.relative_to(self.public_dir)
        
        if data.get('@type') == 'BlogPosting':
            # æª¢æŸ¥ä½œè€…è³‡è¨Šçš„å®Œæ•´æ€§
            author = data.get('author')
            if author and isinstance(author, dict):
                if 'url' not in author:
                    self.suggestions.append(f"{relative_path}: å»ºè­°ç‚ºä½œè€…æ·»åŠ å€‹äººé é¢ URL")
        
        return True
    
    def validate_social_media_optimization(self, data: Dict[Any, Any], file_path: Path) -> bool:
        """é©—è­‰ç¤¾ç¾¤åª’é«”å„ªåŒ–"""
        relative_path = file_path.relative_to(self.public_dir)
        
        if data.get('@type') == 'BlogPosting':
            # æª¢æŸ¥æ˜¯å¦é©åˆç¤¾ç¾¤åˆ†äº«
            if 'image' in data and 'headline' in data and 'description' in data:
                self.success_count += 1
                print(f"âœ… {relative_path}: é©åˆç¤¾ç¾¤åª’é«”åˆ†äº«")
            else:
                self.suggestions.append(f"{relative_path}: å»ºè­°å®Œå–„åœ–ç‰‡ã€æ¨™é¡Œã€æè¿°ä»¥æå‡ç¤¾ç¾¤åˆ†äº«æ•ˆæœ")
        
        return True
    
    def validate_date_consistency(self, data: Dict[Any, Any], file_path: Path) -> bool:
        """é©—è­‰æ—¥æœŸä¸€è‡´æ€§"""
        relative_path = file_path.relative_to(self.public_dir)
        is_valid = True
        
        if data.get('@type') == 'BlogPosting':
            published = data.get('datePublished')
            modified = data.get('dateModified')
            
            if published and modified:
                try:
                    pub_date = datetime.fromisoformat(published.replace('Z', '+00:00'))
                    mod_date = datetime.fromisoformat(modified.replace('Z', '+00:00'))
                    
                    if mod_date < pub_date:
                        self.errors.append(f"{relative_path}: ä¿®æ”¹æ—¥æœŸä¸èƒ½æ—©æ–¼ç™¼å¸ƒæ—¥æœŸ")
                        is_valid = False
                        
                    # æª¢æŸ¥æ—¥æœŸæ˜¯å¦åœ¨æœªä¾†
                    now = datetime.now(pub_date.tzinfo)
                    if pub_date > now:
                        self.warnings.append(f"{relative_path}: ç™¼å¸ƒæ—¥æœŸåœ¨æœªä¾†")
                        
                except Exception as e:
                    self.errors.append(f"{relative_path}: æ—¥æœŸæ ¼å¼éŒ¯èª¤ - {e}")
                    is_valid = False
        
        return is_valid
    
    def generate_optimization_report(self):
        """ç”Ÿæˆå„ªåŒ–å ±å‘Š"""
        report = []
        report.append("# çµæ§‹åŒ–è³‡æ–™å„ªåŒ–å ±å‘Š")
        report.append(f"ç”Ÿæˆæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # çµ±è¨ˆè³‡è¨Š
        report.append("## ğŸ“Š çµ±è¨ˆè³‡è¨Š")
        report.append(f"- âœ… æˆåŠŸé©—è­‰: {self.success_count} å€‹")
        report.append(f"- âŒ éŒ¯èª¤: {len(self.errors)} å€‹")
        report.append(f"- âš ï¸ è­¦å‘Š: {len(self.warnings)} å€‹")
        report.append(f"- ğŸ’¡ å»ºè­°: {len(self.suggestions)} å€‹")
        report.append("")
        
        # éŒ¯èª¤è©³æƒ…
        if self.errors:
            report.append("## âŒ éŒ¯èª¤è©³æƒ…")
            for error in self.errors:
                report.append(f"- {error}")
            report.append("")
        
        # è­¦å‘Šè©³æƒ…
        if self.warnings:
            report.append("## âš ï¸ è­¦å‘Šè©³æƒ…")
            for warning in self.warnings:
                report.append(f"- {warning}")
            report.append("")
        
        # å„ªåŒ–å»ºè­°
        if self.suggestions:
            report.append("## ğŸ’¡ å„ªåŒ–å»ºè­°")
            for suggestion in self.suggestions:
                report.append(f"- {suggestion}")
            report.append("")
        
        # æœ€ä½³å¯¦è¸
        report.append("## ğŸ¯ æœ€ä½³å¯¦è¸å»ºè­°")
        report.append("1. **SEO å„ªåŒ–**:")
        report.append("   - æ¨™é¡Œæ§åˆ¶åœ¨ 30-60 å­—ç¬¦")
        report.append("   - æè¿°æ§åˆ¶åœ¨ 120-160 å­—ç¬¦")
        report.append("   - æ·»åŠ ç›¸é—œé—œéµå­—")
        report.append("")
        report.append("2. **æ•ˆèƒ½å„ªåŒ–**:")
        report.append("   - ä½¿ç”¨ HTTPS åœ–ç‰‡ URL")
        report.append("   - å„ªå…ˆä½¿ç”¨ WebP æ ¼å¼åœ–ç‰‡")
        report.append("   - å£“ç¸®åœ–ç‰‡å¤§å°")
        report.append("")
        report.append("3. **ç¤¾ç¾¤åˆ†äº«**:")
        report.append("   - ç¢ºä¿æ¯ç¯‡æ–‡ç« éƒ½æœ‰ç‰¹è‰²åœ–ç‰‡")
        report.append("   - æ’°å¯«å¸å¼•äººçš„æ¨™é¡Œå’Œæè¿°")
        report.append("   - æ·»åŠ ä½œè€…è³‡è¨Š")
        
        # å„²å­˜å ±å‘Š
        report_path = Path("scripts/optimization-report.md")
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("\n".join(report))
        
        print(f"ğŸ“ å„ªåŒ–å ±å‘Šå·²å„²å­˜è‡³: {report_path}")
    
    def validate_file_enhanced(self, file_path: Path) -> bool:
        """å¢å¼·ç‰ˆæª”æ¡ˆé©—è­‰"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()
        except Exception as e:
            self.errors.append(f"ç„¡æ³•è®€å–æª”æ¡ˆ {file_path}: {e}")
            return False
        
        # æå–çµæ§‹åŒ–è³‡æ–™
        soup = BeautifulSoup(html_content, 'html.parser')
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        
        if not json_ld_scripts:
            return True  # æ²’æœ‰çµæ§‹åŒ–è³‡æ–™ï¼Œè·³é
        
        file_valid = True
        for script in json_ld_scripts:
            try:
                data = json.loads(script.string)
                
                # åŸ·è¡Œå„ç¨®é©—è­‰
                if not self.validate_seo_optimization(data, file_path):
                    file_valid = False
                if not self.validate_performance_optimization(data, file_path):
                    file_valid = False
                if not self.validate_accessibility(data, file_path):
                    file_valid = False
                if not self.validate_social_media_optimization(data, file_path):
                    file_valid = False
                if not self.validate_date_consistency(data, file_path):
                    file_valid = False
                    
            except json.JSONDecodeError as e:
                self.errors.append(f"JSON è§£æéŒ¯èª¤ {file_path}: {e}")
                file_valid = False
        
        return file_valid
    
    def validate_all_enhanced(self) -> bool:
        """å¢å¼·ç‰ˆå…¨æª”æ¡ˆé©—è­‰"""
        if not self.public_dir.exists():
            print(f"âŒ æ‰¾ä¸åˆ° public ç›®éŒ„: {self.public_dir}")
            return False
        
        html_files = []
        for file_path in self.public_dir.rglob("*.html"):
            if file_path.name != "404.html":
                html_files.append(file_path)
        
        print(f"ğŸ” æ‰¾åˆ° {len(html_files)} å€‹ HTML æª”æ¡ˆ")
        print("ğŸš€ é–‹å§‹å¢å¼·ç‰ˆé©—è­‰...")
        
        all_valid = True
        for file_path in html_files:
            if not self.validate_file_enhanced(file_path):
                all_valid = False
        
        return all_valid

def main():
    parser = argparse.ArgumentParser(description='å¢å¼·ç‰ˆçµæ§‹åŒ–è³‡æ–™é©—è­‰å·¥å…·')
    parser.add_argument('--public-dir', default='public', help='public ç›®éŒ„è·¯å¾‘')
    parser.add_argument('--generate-report', action='store_true', help='ç”Ÿæˆå„ªåŒ–å ±å‘Š')
    
    args = parser.parse_args()
    
    validator = EnhancedValidator(args.public_dir)
    
    print("ğŸš€ é–‹å§‹å¢å¼·ç‰ˆçµæ§‹åŒ–è³‡æ–™é©—è­‰...")
    is_valid = validator.validate_all_enhanced()
    
    # å°å‡ºçµæœ
    print("\n" + "="*60)
    print("ğŸ“Š å¢å¼·ç‰ˆé©—è­‰çµæœ")
    print("="*60)
    print(f"âœ… æˆåŠŸé©—è­‰: {validator.success_count} å€‹")
    print(f"âŒ éŒ¯èª¤: {len(validator.errors)} å€‹")
    print(f"âš ï¸ è­¦å‘Š: {len(validator.warnings)} å€‹")
    print(f"ğŸ’¡ å»ºè­°: {len(validator.suggestions)} å€‹")
    
    if args.generate_report:
        validator.generate_optimization_report()
    
    sys.exit(0 if is_valid else 1)

if __name__ == "__main__":
    main()